{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1098dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cpu\n",
      "Collecting gradio\n",
      "  Downloading gradio-5.25.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.8.tar.gz (67.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 MB\u001b[0m \u001b[31m813.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting aiofiles<25.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting anyio<5.0,>=3.0 (from gradio)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.8.0 (from gradio)\n",
      "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting httpx>=0.24.1 (from gradio)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting huggingface-hub>=0.28.1 (from gradio)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jinja2<4.0 (from gradio)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting markupsafe<4.0,>=2.0 (from gradio)\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting numpy<3.0,>=1.0 (from gradio)\n",
      "  Downloading numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.10.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from gradio) (24.2)\n",
      "Collecting pandas<3.0,>=1.0 (from gradio)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting pillow<12.0,>=8.0 (from gradio)\n",
      "  Downloading pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting pydantic<2.12,>=2.0 (from gradio)\n",
      "  Downloading pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pyyaml<7.0,>=5.0 (from gradio)\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting ruff>=0.9.3 (from gradio)\n",
      "  Downloading ruff-0.11.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in ./.venv/lib/python3.10/site-packages (from gradio) (4.13.2)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.34.1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fsspec (from gradio-client==1.8.0->gradio)\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting websockets<16.0,>=10.0 (from gradio-client==1.8.0->gradio)\n",
      "  Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Collecting idna>=2.8 (from anyio<5.0,>=3.0->gradio)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sniffio>=1.1 (from anyio<5.0,>=3.0->gradio)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting certifi (from httpx>=0.24.1->gradio)\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
      "  Downloading httpcore-1.0.8-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.28.1->gradio)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting requests (from huggingface-hub>=0.28.1->gradio)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface-hub>=0.28.1->gradio)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas<3.0,>=1.0->gradio)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas<3.0,>=1.0->gradio)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<2.12,>=2.0->gradio)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic<2.12,>=2.0->gradio)\n",
      "  Downloading pydantic_core-2.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<2.12,>=2.0->gradio)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting click>=8.0.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->huggingface-hub>=0.28.1->gradio)\n",
      "  Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface-hub>=0.28.1->gradio)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading gradio-5.25.2-py3-none-any.whl (46.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m337.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.8-py3-none-any.whl (78 kB)\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Downloading numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m697.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
      "Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m354.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m328.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m491.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m363.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ruff-0.11.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m601.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Downloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Downloading uvicorn-0.34.1-py3-none-any.whl (62 kB)\n",
      "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (181 kB)\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.8-cp310-cp310-linux_x86_64.whl size=6066569 sha256=f6e9e41a4966aa18ce4b04853e546d24430eea15ac5c13bb83f8cafa743d0e6b\n",
      "  Stored in directory: /home/god/.cache/pip/wheels/1e/7f/93/0ba01e12caa1597ca802c25d2445e4af62dbbb630bc05094b0\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: pytz, pydub, websockets, urllib3, tzdata, typing-inspection, tqdm, tomlkit, sniffio, shellingham, semantic-version, ruff, pyyaml, python-multipart, pydantic-core, pillow, orjson, numpy, mdurl, markupsafe, idna, h11, groovy, fsspec, filelock, ffmpy, diskcache, click, charset-normalizer, certifi, annotated-types, aiofiles, uvicorn, requests, pydantic, pandas, markdown-it-py, jinja2, httpcore, anyio, starlette, rich, llama-cpp-python, huggingface-hub, httpx, typer, safehttpx, gradio-client, fastapi, gradio\n",
      "Successfully installed aiofiles-24.1.0 annotated-types-0.7.0 anyio-4.9.0 certifi-2025.1.31 charset-normalizer-3.4.1 click-8.1.8 diskcache-5.6.3 fastapi-0.115.12 ffmpy-0.5.0 filelock-3.18.0 fsspec-2025.3.2 gradio-5.25.2 gradio-client-1.8.0 groovy-0.1.2 h11-0.14.0 httpcore-1.0.8 httpx-0.28.1 huggingface-hub-0.30.2 idna-3.10 jinja2-3.1.6 llama-cpp-python-0.3.8 markdown-it-py-3.0.0 markupsafe-3.0.2 mdurl-0.1.2 numpy-2.2.4 orjson-3.10.16 pandas-2.2.3 pillow-11.2.1 pydantic-2.11.3 pydantic-core-2.33.1 pydub-0.25.1 python-multipart-0.0.20 pytz-2025.2 pyyaml-6.0.2 requests-2.32.3 rich-14.0.0 ruff-0.11.6 safehttpx-0.1.6 semantic-version-2.10.0 shellingham-1.5.4 sniffio-1.3.1 starlette-0.46.2 tomlkit-0.13.2 tqdm-4.67.1 typer-0.15.2 typing-inspection-0.4.0 tzdata-2025.2 urllib3-2.4.0 uvicorn-0.34.1 websockets-15.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gradio llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd451cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/god/SSD/My_Codes/My_AI_Freind/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from qwen2.5-1.5b-instruct-q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = qwen2.5-1.5b-instruct\n",
      "llama_model_loader: - kv   3:                            general.version str              = v0.1\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = qwen2.5-1.5b-instruct\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 1.8B\n",
      "llama_model_loader: - kv   6:                          qwen2.block_count u32              = 28\n",
      "llama_model_loader: - kv   7:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 1536\n",
      "llama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 8960\n",
      "llama_model_loader: - kv  10:                 qwen2.attention.head_count u32              = 12\n",
      "llama_model_loader: - kv  11:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv  12:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  141 tensors\n",
      "llama_model_loader: - type q8_0:  198 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q8_0\n",
      "print_info: file size   = 1.76 GiB (8.50 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.9310 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 1536\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 12\n",
      "print_info: n_head_kv        = 2\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 6\n",
      "print_info: n_embd_k_gqa     = 256\n",
      "print_info: n_embd_v_gqa     = 256\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8960\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 1.5B\n",
      "print_info: model params     = 1.78 B\n",
      "print_info: general.name     = qwen2.5-1.5b-instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151936\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q8_0) (and 338 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  1801.09 MiB\n",
      "............................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 512\n",
      "llama_init_from_model: n_ctx_per_seq = 512\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 1000000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init:        CPU KV buffer size =    14.00 MiB\n",
      "llama_init_from_model: KV self size  =   14.00 MiB, K (f16):    7.00 MiB, V (f16):    7.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   299.75 MiB\n",
      "llama_init_from_model: graph nodes  = 986\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.bos_token_id': '151643', 'general.file_type': '7', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.architecture': 'qwen2', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2.embedding_length': '1536', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'qwen2.5-1.5b-instruct', 'qwen2.block_count': '28', 'general.version': 'v0.1', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000', 'general.finetune': 'qwen2.5-1.5b-instruct', 'general.type': 'model', 'general.size_label': '1.8B', 'qwen2.context_length': '32768', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'qwen2.attention.head_count_kv': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.feed_forward_length': '8960', 'qwen2.attention.head_count': '12'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import gradio as gr\n",
    "\n",
    "llm = Llama(\n",
    "    model_path = \"qwen2.5-1.5b-instruct-q8_0.gguf\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7706c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "    for user_message, bot_message in history: \n",
    "        if user_message: \n",
    "            messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        if bot_message:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": bot_message})\n",
    "\n",
    "    # Add the new user message\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    response = \"\"\n",
    "    try:\n",
    "        for chunks in llm.create_chat_completion(stream=True, messages=messages):\n",
    "            part = chunks[\"choices\"][0][\"delta\"].get(\"content\", None)\n",
    "            if part:\n",
    "                response += part\n",
    "                yield response\n",
    "    except Exception as e:\n",
    "        # Ensure at least something is yielded to prevent StopAsyncIteration crash\n",
    "        yield \"An error occurred while generating a response.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fefb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/god/SSD/My_Codes/My_AI_Freind/.venv/lib/python3.10/site-packages/gradio/chat_interface.py:338: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     536.81 ms\n",
      "llama_perf_context_print: prompt eval time =     536.58 ms /    20 tokens (   26.83 ms per token,    37.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =     880.78 ms /     9 runs   (   97.86 ms per token,    10.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1442.49 ms /    29 tokens\n",
      "Llama.generate: 29 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     536.81 ms\n",
      "llama_perf_context_print: prompt eval time =     617.03 ms /    19 tokens (   32.48 ms per token,    30.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4753.97 ms /    49 runs   (   97.02 ms per token,    10.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    5497.09 ms /    68 tokens\n",
      "Llama.generate: 97 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     536.81 ms\n",
      "llama_perf_context_print: prompt eval time =     743.06 ms /    24 tokens (   30.96 ms per token,    32.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2907.16 ms /    29 runs   (  100.25 ms per token,     9.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    3724.39 ms /    53 tokens\n",
      "Llama.generate: 150 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     536.81 ms\n",
      "llama_perf_context_print: prompt eval time =     594.57 ms /    17 tokens (   34.97 ms per token,    28.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10446.36 ms /   104 runs   (  100.45 ms per token,     9.96 tokens per second)\n",
      "llama_perf_context_print:       total time =   11325.17 ms /   121 tokens\n",
      "Llama.generate: 271 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     536.81 ms\n",
      "llama_perf_context_print: prompt eval time =     501.57 ms /    14 tokens (   35.83 ms per token,    27.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19701.45 ms /   197 runs   (  100.01 ms per token,    10.00 tokens per second)\n",
      "llama_perf_context_print:       total time =   20758.18 ms /   211 tokens\n",
      "Llama.generate: 482 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     536.81 ms\n",
      "llama_perf_context_print: prompt eval time =     509.96 ms /    14 tokens (   36.43 ms per token,    27.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1529.12 ms /    15 runs   (  101.94 ms per token,     9.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    2082.97 ms /    29 tokens\n",
      "Llama.generate: 495 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     536.81 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1693.43 ms /    16 runs   (  105.84 ms per token,     9.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1737.40 ms /    17 tokens\n"
     ]
    }
   ],
   "source": [
    "demo = gr.ChatInterface(predict)\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eabf21ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting whispercpp\n",
      "  Downloading whispercpp-0.0.17-cp310-cp310-manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting sounddevice\n",
      "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (2.2.4)\n",
      "Collecting CFFI>=1.0 (from sounddevice)\n",
      "  Using cached cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting pycparser (from CFFI>=1.0->sounddevice)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Downloading whispercpp-0.0.17-cp310-cp310-manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m248.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
      "Using cached cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (446 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: whispercpp, pycparser, CFFI, sounddevice\n",
      "Successfully installed CFFI-1.17.1 pycparser-2.22 sounddevice-0.5.1 whispercpp-0.0.17\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install whispercpp sounddevice numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a553c9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from '/home/god/.local/share/pywhispercpp/models/ggml-tiny.en.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51864\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 384\n",
      "whisper_model_load: n_audio_head  = 6\n",
      "whisper_model_load: n_audio_layer = 4\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 384\n",
      "whisper_model_load: n_text_head   = 6\n",
      "whisper_model_load: n_text_layer  = 4\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 1 (tiny)\n",
      "whisper_model_load: adding 1607 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:      CPU total size =    77.11 MB\n",
      "whisper_model_load: model size    =   77.11 MB\n",
      "whisper_init_state: kv self size  =    9.44 MB\n",
      "whisper_init_state: kv cross size =    9.44 MB\n",
      "whisper_init_state: kv pad  size  =    2.36 MB\n",
      "whisper_init_state: compute buffer (conv)   =   13.19 MB\n",
      "whisper_init_state: compute buffer (encode) =   85.53 MB\n",
      "whisper_init_state: compute buffer (cross)  =    3.88 MB\n",
      "whisper_init_state: compute buffer (decode) =   95.89 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored from cffi callback <function _StreamBase.__init__.<locals>.callback_ptr at 0x79dbda7db520>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/god/SSD/My_Codes/My_AI_Freind/.venv/lib/python3.10/site-packages/sounddevice.py\", line 857, in callback_ptr\n",
      "    return _wrap_callback(callback, data, frames, time, status)\n",
      "  File \"/media/god/SSD/My_Codes/My_AI_Freind/.venv/lib/python3.10/site-packages/sounddevice.py\", line 2735, in _wrap_callback\n",
      "    callback(*args)\n",
      "  File \"/tmp/ipykernel_74681/581923487.py\", line 27, in audio_callback\n",
      "  File \"/tmp/ipykernel_74681/581923487.py\", line 34, in transcribe_audio\n",
      "  File \"/media/god/SSD/My_Codes/My_AI_Freind/.venv/lib/python3.10/site-packages/pywhispercpp/model.py\", line 119, in transcribe\n",
      "    if not Path(media).exists():\n",
      "  File \"/home/god/.pyenv/versions/3.10.13/lib/python3.10/pathlib.py\", line 960, in __new__\n",
      "    self = cls._from_parts(args)\n",
      "  File \"/home/god/.pyenv/versions/3.10.13/lib/python3.10/pathlib.py\", line 594, in _from_parts\n",
      "    drv, root, parts = self._parse_args(args)\n",
      "  File \"/home/god/.pyenv/versions/3.10.13/lib/python3.10/pathlib.py\", line 583, in _parse_args\n",
      "    raise TypeError(\n",
      "TypeError: argument should be a str object or an os.PathLike object returning str, not <class 'bytes'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 1.3754569292068481\n",
      "Energy level: 0.158638134598732\n",
      "Speech detected. Transcribing...\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from pywhispercpp.model import Model\n",
    "\n",
    "whisper_model = Model('tiny.en', n_threads=6)\n",
    "\n",
    "samplerate = 16000 #16KHZ\n",
    "channels = 1 # Mono\n",
    "block_duration = 1.0 # 1 second\n",
    "block_size = int(samplerate * block_duration)\n",
    "\n",
    "def is_speech(audio_data, threshold=0.01):  # Try a lower threshold\n",
    "    energy = np.abs(audio_data).mean()  # Different way to calculate energy\n",
    "    print(f\"Energy level: {energy}\")  # Add this to see actual levels\n",
    "    return energy > threshold\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    if status:\n",
    "        print(f\"Status: {status}\")  # Print any status messages\n",
    "    \n",
    "    audio_data = indata[:,0]\n",
    "    max_amplitude = np.max(np.abs(audio_data))\n",
    "    print(f\"Max amplitude: {max_amplitude}\")  # See if mic is picking up anything\n",
    "    \n",
    "    if is_speech(audio_data):\n",
    "        print(\"Speech detected. Transcribing...\")\n",
    "        transcribe_audio(audio_data)\n",
    "    else:\n",
    "        print(\"Silence detected\")\n",
    "\n",
    "\n",
    "def transcribe_audio(audio_data):\n",
    "    audio_bytes = audio_data.tobytes()\n",
    "    result = whisper_model.transcribe(audio_bytes)\n",
    "    print(\"Transcription:\", result['text'])\n",
    "\n",
    "#Start the audio stream\n",
    "with sd.InputStream(callback=audio_callback, channels=channels, samplerate = samplerate, blocksize=block_size):\n",
    "    print(\"Listening...\")\n",
    "    sd.sleep(60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f278983f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from '/home/god/.local/share/pywhispercpp/models/ggml-tiny.en.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51864\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 384\n",
      "whisper_model_load: n_audio_head  = 6\n",
      "whisper_model_load: n_audio_layer = 4\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 384\n",
      "whisper_model_load: n_text_head   = 6\n",
      "whisper_model_load: n_text_layer  = 4\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 1 (tiny)\n",
      "whisper_model_load: adding 1607 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:      CPU total size =    77.11 MB\n",
      "whisper_model_load: model size    =   77.11 MB\n",
      "whisper_init_state: kv self size  =    9.44 MB\n",
      "whisper_init_state: kv cross size =    9.44 MB\n",
      "whisper_init_state: kv pad  size  =    2.36 MB\n",
      "whisper_init_state: compute buffer (conv)   =   13.19 MB\n",
      "whisper_init_state: compute buffer (encode) =   85.53 MB\n",
      "whisper_init_state: compute buffer (cross)  =    3.88 MB\n",
      "whisper_init_state: compute buffer (decode) =   95.89 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "Max amplitude: 0.30609095096588135\n",
      "Energy level: 0.08688570559024811\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 0.2664104700088501\n",
      "Energy level: 0.07150177657604218\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 0.35715991258621216\n",
      "Energy level: 0.1277047097682953\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 0.27119290828704834\n",
      "Energy level: 0.07237724214792252\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 0.5645263195037842\n",
      "Energy level: 0.1291026622056961\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 0.4547690153121948\n",
      "Energy level: 0.09562992304563522\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 0.3900664746761322\n",
      "Energy level: 0.12879464030265808\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 0.2743533253669739\n",
      "Energy level: 0.07472139596939087\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 0.6083381175994873\n",
      "Energy level: 0.06780318915843964\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 0.23788240551948547\n",
      "Energy level: 0.05652869492769241\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 0.24363498389720917\n",
      "Energy level: 0.06000138819217682\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 0.23772470653057098\n",
      "Energy level: 0.05774432420730591\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 0.22927382588386536\n",
      "Energy level: 0.053351029753685\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 0.2079392671585083\n",
      "Energy level: 0.05146528780460358\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 0.2430177479982376\n",
      "Energy level: 0.05716770142316818\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 0.2382938265800476\n",
      "Energy level: 0.05963104963302612\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 0.2266986072063446\n",
      "Energy level: 0.04749394953250885\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 0.2313036024570465\n",
      "Energy level: 0.06181464344263077\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 0.20886336266994476\n",
      "Energy level: 0.04871145263314247\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 0.36420369148254395\n",
      "Energy level: 0.11005312949419022\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 0.258140504360199\n",
      "Energy level: 0.06742623448371887\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 0.3059999942779541\n",
      "Energy level: 0.09741701185703278\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max amplitude: 0.31905579566955566\n",
      "Energy level: 0.1058172658085823\n",
      "Speech detected. Transcribing...\n",
      "Transcription error: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: input is too short - 990 ms < 1000 ms. consider padding the input audio with silence\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import os\n",
    "from pywhispercpp.model import Model\n",
    "\n",
    "whisper_model = Model('tiny.en', n_threads=6)\n",
    "\n",
    "samplerate = 16000  # 16KHZ\n",
    "channels = 1  # Mono\n",
    "block_duration = 1.0  # 1 second\n",
    "block_size = int(samplerate * block_duration)\n",
    "\n",
    "def is_speech(audio_data, threshold=0.01):\n",
    "    energy = np.abs(audio_data).mean()\n",
    "    print(f\"Energy level: {energy}\")  # Debug info\n",
    "    return energy > threshold\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    if status:\n",
    "        print(f\"Status: {status}\")  # Print any status issues\n",
    "        \n",
    "    audio_data = indata[:,0]\n",
    "    max_amplitude = np.max(np.abs(audio_data))\n",
    "    print(f\"Max amplitude: {max_amplitude}\")  # Debug info\n",
    "    \n",
    "    if is_speech(audio_data):\n",
    "        print(\"Speech detected. Transcribing...\")\n",
    "        transcribe_audio(audio_data)\n",
    "    else:\n",
    "        print(\"Silence detected\")\n",
    "\n",
    "def transcribe_audio(audio_data):\n",
    "    # Convert float32 audio data to int16 (typical for WAV files)\n",
    "    audio_int16 = (audio_data * 32767).astype(np.int16)\n",
    "    \n",
    "    # Create a temporary WAV file\n",
    "    with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:\n",
    "        temp_filename = temp_file.name\n",
    "        \n",
    "    # Save the audio data to the temporary file\n",
    "    import wave\n",
    "    with wave.open(temp_filename, 'wb') as wf:\n",
    "        wf.setnchannels(channels)\n",
    "        wf.setsampwidth(2)  # 2 bytes for int16\n",
    "        wf.setframerate(samplerate)\n",
    "        wf.writeframes(audio_int16.tobytes())\n",
    "    \n",
    "    try:\n",
    "        # Now pass the file path to the transcribe function\n",
    "        result = whisper_model.transcribe(temp_filename)\n",
    "        print(\"Transcription:\", result['text'])\n",
    "    except Exception as e:\n",
    "        print(f\"Transcription error: {e}\")\n",
    "    finally:\n",
    "        # Clean up the temporary file\n",
    "        os.unlink(temp_filename)\n",
    "\n",
    "# Start the audio stream\n",
    "print(\"Listening...\")\n",
    "with sd.InputStream(callback=audio_callback, channels=channels, samplerate=samplerate, blocksize=block_size):\n",
    "    sd.sleep(60000)  # Listen for 60 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6091517",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspeech_recognition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msr\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mqueue\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Queue\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "#! python3.7\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "import torch\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from queue import Queue\n",
    "from time import sleep\n",
    "from sys import platform\n",
    "from pywhispercpp.model import Model\n",
    "\n",
    "whisper_model = Model('tiny.en', n_threads=6)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model\", default=\"tiny\", help=\"Model to use\",\n",
    "                        choices=[\"tiny\", \"base\", \"small\", \"medium\", \"large\"])\n",
    "    parser.add_argument(\"--non_english\", action='store_true',\n",
    "                        help=\"Don't use the english model.\")\n",
    "    parser.add_argument(\"--energy_threshold\", default=1000,\n",
    "                        help=\"Energy level for mic to detect.\", type=int)\n",
    "    parser.add_argument(\"--record_timeout\", default=2,\n",
    "                        help=\"How real time the recording is in seconds.\", type=float)\n",
    "    parser.add_argument(\"--phrase_timeout\", default=3,\n",
    "                        help=\"How much empty space between recordings before we \"\n",
    "                             \"consider it a new line in the transcription.\", type=float)\n",
    "    if 'linux' in platform:\n",
    "        parser.add_argument(\"--default_microphone\", default='pulse',\n",
    "                            help=\"Default microphone name for SpeechRecognition. \"\n",
    "                                 \"Run this with 'list' to view available Microphones.\", type=str)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # The last time a recording was retrieved from the queue.\n",
    "    phrase_time = None\n",
    "    # Thread safe Queue for passing data from the threaded recording callback.\n",
    "    data_queue = Queue()\n",
    "    # Bytes object which holds audio data for the current phrase\n",
    "    phrase_bytes = bytes()\n",
    "    # We use SpeechRecognizer to record our audio because it has a nice feature where it can detect when speech ends.\n",
    "    recorder = sr.Recognizer()\n",
    "    recorder.energy_threshold = args.energy_threshold\n",
    "    # Definitely do this, dynamic energy compensation lowers the energy threshold dramatically to a point where the SpeechRecognizer never stops recording.\n",
    "    recorder.dynamic_energy_threshold = False\n",
    "\n",
    "    # Important for linux users.\n",
    "    # Prevents permanent application hang and crash by using the wrong Microphone\n",
    "    if 'linux' in platform:\n",
    "        mic_name = args.default_microphone\n",
    "        if not mic_name or mic_name == 'list':\n",
    "            print(\"Available microphone devices are: \")\n",
    "            for index, name in enumerate(sr.Microphone.list_microphone_names()):\n",
    "                print(f\"Microphone with name \\\"{name}\\\" found\")\n",
    "            return\n",
    "        else:\n",
    "            for index, name in enumerate(sr.Microphone.list_microphone_names()):\n",
    "                if mic_name in name:\n",
    "                    source = sr.Microphone(sample_rate=16000, device_index=index)\n",
    "                    break\n",
    "    else:\n",
    "        source = sr.Microphone(sample_rate=16000)\n",
    "\n",
    "    # Load / Download model\n",
    "    model = args.model\n",
    "    if args.model != \"large\" and not args.non_english:\n",
    "        model = model + \".en\"\n",
    "    audio_model = Model('tiny.en', n_threads=6)\n",
    "\n",
    "    record_timeout = args.record_timeout\n",
    "    phrase_timeout = args.phrase_timeout\n",
    "\n",
    "    transcription = ['']\n",
    "\n",
    "    with source:\n",
    "        recorder.adjust_for_ambient_noise(source)\n",
    "\n",
    "    def record_callback(_, audio:sr.AudioData) -> None:\n",
    "        \"\"\"\n",
    "        Threaded callback function to receive audio data when recordings finish.\n",
    "        audio: An AudioData containing the recorded bytes.\n",
    "        \"\"\"\n",
    "        # Grab the raw bytes and push it into the thread safe queue.\n",
    "        data = audio.get_raw_data()\n",
    "        data_queue.put(data)\n",
    "\n",
    "    # Create a background thread that will pass us raw audio bytes.\n",
    "    # We could do this manually but SpeechRecognizer provides a nice helper.\n",
    "    recorder.listen_in_background(source, record_callback, phrase_time_limit=record_timeout)\n",
    "\n",
    "    # Cue the user that we're ready to go.\n",
    "    print(\"Model loaded.\\n\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            now = datetime.utcnow()\n",
    "            # Pull raw recorded audio from the queue.\n",
    "            if not data_queue.empty():\n",
    "                phrase_complete = False\n",
    "                # If enough time has passed between recordings, consider the phrase complete.\n",
    "                # Clear the current working audio buffer to start over with the new data.\n",
    "                if phrase_time and now - phrase_time > timedelta(seconds=phrase_timeout):\n",
    "                    phrase_bytes = bytes()\n",
    "                    phrase_complete = True\n",
    "                # This is the last time we received new audio data from the queue.\n",
    "                phrase_time = now\n",
    "                \n",
    "                # Combine audio data from queue\n",
    "                audio_data = b''.join(data_queue.queue)\n",
    "                data_queue.queue.clear()\n",
    "\n",
    "                # Add the new audio data to the accumulated data for this phrase\n",
    "                phrase_bytes += audio_data\n",
    "\n",
    "                # Convert in-ram buffer to something the model can use directly without needing a temp file.\n",
    "                # Convert data from 16 bit wide integers to floating point with a width of 32 bits.\n",
    "                # Clamp the audio stream frequency to a PCM wavelength compatible default of 32768hz max.\n",
    "                audio_np = np.frombuffer(phrase_bytes, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "\n",
    "                # Read the transcription.\n",
    "                result = audio_model.transcribe(audio_np, fp16=torch.cuda.is_available())\n",
    "                text = result['text'].strip()\n",
    "\n",
    "                # If we detected a pause between recordings, add a new item to our transcription.\n",
    "                # Otherwise edit the existing one.\n",
    "                if phrase_complete:\n",
    "                    transcription.append(text)\n",
    "                else:\n",
    "                    transcription[-1] = text\n",
    "\n",
    "                # Clear the console to reprint the updated transcription.\n",
    "                os.system('cls' if os.name=='nt' else 'clear')\n",
    "                for line in transcription:\n",
    "                    print(line)\n",
    "                # Flush stdout.\n",
    "                print('', end='', flush=True)\n",
    "            else:\n",
    "                # Infinite loops are bad for processors, must sleep.\n",
    "                sleep(0.25)\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "    print(\"\\n\\nTranscription:\")\n",
    "    for line in transcription:\n",
    "        print(line)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d8ee56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in ./.venv/lib/python3.10/site-packages (3.14.2)\n",
      "Collecting torch\n"
     ]
    }
   ],
   "source": [
    "!pip install SpeechRecognition torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c7c7ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from '/home/god/.local/share/pywhispercpp/models/ggml-tiny.en.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51864\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 384\n",
      "whisper_model_load: n_audio_head  = 6\n",
      "whisper_model_load: n_audio_layer = 4\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 384\n",
      "whisper_model_load: n_text_head   = 6\n",
      "whisper_model_load: n_text_layer  = 4\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 1 (tiny)\n",
      "whisper_model_load: adding 1607 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:      CPU total size =    77.11 MB\n",
      "whisper_model_load: model size    =   77.11 MB\n",
      "whisper_init_state: kv self size  =    9.44 MB\n",
      "whisper_init_state: kv cross size =    9.44 MB\n",
      "whisper_init_state: kv pad  size  =    2.36 MB\n",
      "whisper_init_state: compute buffer (conv)   =   13.19 MB\n",
      "whisper_init_state: compute buffer (encode) =   85.53 MB\n",
      "whisper_init_state: compute buffer (cross)  =    3.88 MB\n",
      "whisper_init_state: compute buffer (decode) =   95.89 MB\n",
      "Expression 'paInvalidSampleRate' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 2048\n",
      "Expression 'PaAlsaStreamComponent_InitialConfigure( &self->capture, inParams, self->primeBuffers, hwParamsCapture, &realSr )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 2718\n",
      "Expression 'PaAlsaStream_Configure( stream, inputParameters, outputParameters, sampleRate, framesPerBuffer, &inputLatency, &outputLatency, &hostBufferSizeMode )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 2842\n"
     ]
    },
    {
     "ename": "PortAudioError",
     "evalue": "Error opening InputStream: Invalid sample rate [PaErrorCode -9997]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPortAudioError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Use the earphone mic device\u001b[39;00m\n\u001b[1;32m     31\u001b[0m input_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# ALC887-VD Alt Analog\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInputStream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfloat32\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎧 Listening for 1 minute... Speak into the mic!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m     sd\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m60000\u001b[39m)\n",
      "File \u001b[0;32m/media/god/SSD/My_Codes/My_AI_Freind/.venv/lib/python3.10/site-packages/sounddevice.py:1440\u001b[0m, in \u001b[0;36mInputStream.__init__\u001b[0;34m(self, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback)\u001b[0m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, samplerate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1409\u001b[0m              device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, latency\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1410\u001b[0m              extra_settings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, finished_callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1411\u001b[0m              clip_off\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither_off\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, never_drop_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1412\u001b[0m              prime_output_buffers_using_stream_callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1413\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"PortAudio input stream (using NumPy).\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m \n\u001b[1;32m   1415\u001b[0m \u001b[38;5;124;03m    This has the same methods and attributes as `Stream`, except\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \n\u001b[1;32m   1439\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1440\u001b[0m     \u001b[43m_StreamBase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrap_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marray\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1441\u001b[0m \u001b[43m                         \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_remove_self\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/god/SSD/My_Codes/My_AI_Freind/.venv/lib/python3.10/site-packages/sounddevice.py:909\u001b[0m, in \u001b[0;36m_StreamBase.__init__\u001b[0;34m(self, kind, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback, userdata, wrap_callback)\u001b[0m\n\u001b[1;32m    907\u001b[0m     userdata \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mNULL\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ptr \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPaStream**\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 909\u001b[0m \u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_OpenStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m                          \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_flags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcallback_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mError opening \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;66;03m# dereference PaStream** --> PaStream*\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ptr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ptr[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/media/god/SSD/My_Codes/My_AI_Freind/.venv/lib/python3.10/site-packages/sounddevice.py:2796\u001b[0m, in \u001b[0;36m_check\u001b[0;34m(err, msg)\u001b[0m\n\u001b[1;32m   2793\u001b[0m     hosterror_info \u001b[38;5;241m=\u001b[39m host_api, info\u001b[38;5;241m.\u001b[39merrorCode, hosterror_text\n\u001b[1;32m   2794\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PortAudioError(errormsg, err, hosterror_info)\n\u001b[0;32m-> 2796\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m PortAudioError(errormsg, err)\n",
      "\u001b[0;31mPortAudioError\u001b[0m: Error opening InputStream: Invalid sample rate [PaErrorCode -9997]"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from pywhispercpp.model import Model\n",
    "\n",
    "whisper_model = Model('tiny.en', n_threads=6)\n",
    "\n",
    "samplerate = 16000\n",
    "channels = 1\n",
    "block_duration = 1.0\n",
    "block_size = int(samplerate * block_duration)\n",
    "\n",
    "def is_speech(audio_data, threshold=0.01):\n",
    "    energy = np.linalg.norm(audio_data) / len(audio_data)\n",
    "    print(f\"Energy: {energy:.5f}\")  # For debugging\n",
    "    return energy > threshold\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    audio_data = indata[:, 0]\n",
    "    if is_speech(audio_data):\n",
    "        print(\"🎤 Speech detected! Transcribing...\")\n",
    "        transcribe_audio(audio_data)\n",
    "    else:\n",
    "        print(\"...Silence\")\n",
    "\n",
    "def transcribe_audio(audio_data):\n",
    "    audio_bytes = audio_data.tobytes()\n",
    "    result = whisper_model.transcribe(audio_bytes)\n",
    "    print(\"📝 Transcription:\", result['text'])\n",
    "\n",
    "# Use the earphone mic device\n",
    "input_device = 3  # ALC887-VD Alt Analog\n",
    "\n",
    "with sd.InputStream(callback=audio_callback,\n",
    "                    device=input_device,\n",
    "                    channels=channels,\n",
    "                    samplerate=samplerate,\n",
    "                    blocksize=block_size,\n",
    "                    dtype='float32'):\n",
    "    print(\"🎧 Listening for 1 minute... Speak into the mic!\")\n",
    "    sd.sleep(60000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8365be66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Model base.en ...:   2%|▏         | 3.11M/141M [00:11<08:21, 289kiB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpywhispercpp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbase.en\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m segments \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtranscribe(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile.mp3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m segment \u001b[38;5;129;01min\u001b[39;00m segments:\n",
      "File \u001b[0;32m/media/god/SSD/My_Codes/My_AI_Freind/.venv/lib/python3.10/site-packages/pywhispercpp/model.py:87\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, model, models_dir, params_sampling_strategy, redirect_whispercpp_logs_to, **params)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_strategy \u001b[38;5;241m=\u001b[39m pw\u001b[38;5;241m.\u001b[39mwhisper_sampling_strategy\u001b[38;5;241m.\u001b[39mWHISPER_SAMPLING_GREEDY \u001b[38;5;28;01mif\u001b[39;00m params_sampling_strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \\\n\u001b[1;32m     90\u001b[0m     pw\u001b[38;5;241m.\u001b[39mwhisper_sampling_strategy\u001b[38;5;241m.\u001b[39mWHISPER_SAMPLING_BEAM_SEARCH\n",
      "File \u001b[0;32m/media/god/SSD/My_Codes/My_AI_Freind/.venv/lib/python3.10/site-packages/pywhispercpp/utils.py:65\u001b[0m, in \u001b[0;36mdownload_model\u001b[0;34m(model_name, download_dir, chunk_size)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file, progress_bar:\n\u001b[0;32m---> 65\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size):\n\u001b[1;32m     66\u001b[0m             size \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[1;32m     67\u001b[0m             progress_bar\u001b[38;5;241m.\u001b[39mupdate(size)\n",
      "File \u001b[0;32m/media/god/SSD/My_Codes/My_AI_Freind/.venv/lib/python3.10/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/media/god/SSD/My_Codes/My_AI_Freind/.venv/lib/python3.10/site-packages/urllib3/response.py:1066\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1066\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1069\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m/media/god/SSD/My_Codes/My_AI_Freind/.venv/lib/python3.10/site-packages/urllib3/response.py:955\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 955\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/media/god/SSD/My_Codes/My_AI_Freind/.venv/lib/python3.10/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    876\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 879\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/media/god/SSD/My_Codes/My_AI_Freind/.venv/lib/python3.10/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pywhispercpp.model import Model\n",
    "\n",
    "model = Model('tiny.en', n_threads=6)\n",
    "segments = model.transcribe('file.mp3')\n",
    "for segment in segments:\n",
    "    print(segment.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
